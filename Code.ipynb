{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\garvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\garvi/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:03<00:00, 14.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "------------------------------\n",
      "train Loss: 0.7212 Acc: 0.6375\n",
      "val Loss: 1.5310 Acc: 0.6375\n",
      "Epoch 2/10\n",
      "------------------------------\n",
      "train Loss: 0.5480 Acc: 0.7406\n",
      "val Loss: 2.3427 Acc: 0.6725\n",
      "Epoch 3/10\n",
      "------------------------------\n",
      "train Loss: 0.4866 Acc: 0.7812\n",
      "val Loss: 0.6100 Acc: 0.6825\n",
      "Epoch 4/10\n",
      "------------------------------\n",
      "train Loss: 0.4341 Acc: 0.8150\n",
      "val Loss: 0.4789 Acc: 0.8275\n",
      "Epoch 5/10\n",
      "------------------------------\n",
      "train Loss: 0.4138 Acc: 0.8300\n",
      "val Loss: 0.4775 Acc: 0.7625\n",
      "Epoch 6/10\n",
      "------------------------------\n",
      "train Loss: 0.3803 Acc: 0.8400\n",
      "val Loss: 0.3790 Acc: 0.8550\n",
      "Epoch 7/10\n",
      "------------------------------\n",
      "train Loss: 0.3389 Acc: 0.8725\n",
      "val Loss: 0.2958 Acc: 0.8575\n",
      "Epoch 8/10\n",
      "------------------------------\n",
      "train Loss: 0.3072 Acc: 0.8762\n",
      "val Loss: 0.2650 Acc: 0.8850\n",
      "Epoch 9/10\n",
      "------------------------------\n",
      "train Loss: 0.2999 Acc: 0.8862\n",
      "val Loss: 0.2674 Acc: 0.8900\n",
      "Epoch 10/10\n",
      "------------------------------\n",
      "train Loss: 0.2680 Acc: 0.8975\n",
      "val Loss: 0.3721 Acc: 0.8650\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset class for preprocessing\n",
    "class ViolenceDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Read video and extract frames\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < 16:  # Fixed number of frames per video\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # If not enough frames, pad with black frames\n",
    "        while len(frames) < 16:\n",
    "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "        # Convert to tensor\n",
    "        frames = np.stack(frames, axis=0)  # Shape: (16, 224, 224, 3)\n",
    "        frames = frames.transpose((0, 3, 1, 2))  # Shape: (16, 3, 224, 224)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32) / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "# Load dataset paths and labels\n",
    "def load_dataset(root_dir):\n",
    "    violence_dir = os.path.join(root_dir, \"Violence\")\n",
    "    non_violence_dir = os.path.join(root_dir, \"NonViolence\")\n",
    "\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(violence_dir):\n",
    "        video_paths.append(os.path.join(violence_dir, file))\n",
    "        labels.append(1)  # Violence = 1\n",
    "\n",
    "    for file in os.listdir(non_violence_dir):\n",
    "        video_paths.append(os.path.join(non_violence_dir, file))\n",
    "        labels.append(0)  # Non-violence = 0\n",
    "\n",
    "    return video_paths, labels\n",
    "\n",
    "# Define model\n",
    "def build_model():\n",
    "    base_model = models.resnet18(pretrained=True)\n",
    "    base_model.fc = nn.Linear(base_model.fc.in_features, 2)  # Binary classification\n",
    "    return base_model\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs[:, 0, :, :, :])  # Use the first frame for simplicity\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"dataset/RealLifeViolenceDataset\"\n",
    "    video_paths, labels = load_dataset(root_dir)\n",
    "\n",
    "    # Train-test split\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        video_paths, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ViolenceDataset(train_paths, train_labels, transform=transform)\n",
    "    val_dataset = ViolenceDataset(val_paths, val_labels, transform=transform)\n",
    "\n",
    "    dataloaders = {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=8, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=8, shuffle=False),\n",
    "    }\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = build_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model\n",
    "    trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), \"violence_model.pth\")\n",
    "    print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\garvi\\AppData\\Local\\Temp\\ipykernel_3728\\809058327.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model structure (same as the one used for training)\n",
    "def build_model():\n",
    "    base_model = models.resnet18(pretrained=False)  # `pretrained=False` to avoid downloading weights again\n",
    "    base_model.fc = nn.Linear(base_model.fc.in_features, 2)  # Binary classification\n",
    "    return base_model\n",
    "\n",
    "# Load the model\n",
    "model_path = \"violence_model.pth\"\n",
    "model = build_model()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
